<!--
#   VRxAR Labs | ODXVRxNTS 360 Viewer
#
#   Author: Nico Reski
#   Email: nico.reski@lnu.se
#   Web: https://vrxar.lnu.se/apps/odxvrxnts-360/
#
#   Pannellum version: 2.4.0
-->

<!DOCTYPE HTML>

<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ODXVRxNTS: 360 Viewer - VRxAR Labs</title>
    <link rel="stylesheet" href="lib/pannellum.css"/>
    <script type="text/javascript" src="lib/pannellum.js" charset="utf-8"></script>

    <style type="text/css">
        #panorama {
            width:  100%;
            height: 500px;
        }

        /* Panorama element size adjustment based on device's screen size. */
        @media not screen and (max-width: 1024px) {
            #panorama {
                height: 576px;
            }
        }

        @media not screen and (max-width: 1280px) {
            #panorama {
                height: 720px;
            }
        }

        @media not screen and (max-width: 1600px) {
            #panorama {
                height: 900px;
            }
        }

        @media not screen and (max-width: 1920px) {
            #panorama {
                height: 1080px;
            }
        }
    </style>
</head>

<body>
    <div id="panorama"></div>

    <script>

        // determine which image to load via url parameter
        //

        var appURLString = window.location.href;        // grab url that was used to open this web app
        var appURL = new URL(appURLString);             // create URL object
        var ino = appURL.searchParams.get("ino");       // extract url argument "ino" (= image number)
        var imageFilePath = "";                         // string to store file path of loaded image
        switch(ino)
        {
            case "1":
                imageFilePath = "images/2019-01-25_ODXVRxNTS_360_01.jpg";
                break;
            case "2":
                imageFilePath = "images/2019-01-25_ODXVRxNTS_360_02.jpg";
                break;
            case "3":
                imageFilePath = "images/2019-01-25_ODXVRxNTS_360_03.jpg";
                break;
            case "4":
                imageFilePath = "images/2019-01-25_ODXVRxNTS_360_04.jpg";
                break;
            case "5":
                imageFilePath = "images/2019-01-25_ODXVRxNTS_360_05.jpg";
                break;
            case "6":
                imageFilePath = "images/2019-01-25_ODXVRxNTS_360_06.jpg";
                break;
            case "8":
                imageFilePath = "images/2019-01-25_ODXVRxNTS_360_09.jpg";
                break;
            case "9":
                imageFilePath = "images/2019-01-25_ODXVRxNTS_360_09.jpg";
                break;
            case "7":
            default:
                imageFilePath = "images/2019-01-25_ODXVRxNTS_360_07.jpg";
                break;
        }


        // prepare hotspot data
        //
        
        var hotspotData = null;                                 // variable potentially holding hotspot data
        var annotated = appURL.searchParams.get("annot");       // extract url argument "annot" (= annotation)
        if (annotated != "false")
        {
            imageFilePath = "images/2019-01-25_ODXVRxNTS_360_07.jpg";  // annotation hotspots are only implemented for file images/2019-01-25_ODXVRxNTS_360_07.jpg

            // hotspot data for file: images/2019-01-25_ODXVRxNTS_360_07.jpg
            hotspotData = [
                {
                    "pitch": -10.0,
                    "yaw": -133.0,
                    "type": "info",
                    "text": "3D gestural input: The hands of the VR user are detected, then displayed, and translated into the virtual 3D space accordingly, live and in real-time, enabling interaction using hand postures and gestures."
                },
                {
                    "pitch": -25.0,
                    "yaw": -105.0,
                    "type": "info",
                    "text": "Time Menu: Attached to the VR user's left-hand palm is a menu with two buttons, allowing the VR user to explore the displayed dataset forward and backward in time."
                },
                {
                    "pitch": 6.0,
                    "yaw": -94.0,
                    "type": "info",
                    "text": "Information Panel - Center: Display the detailed language distribution of the selected cluster (language name and its assigned color, share in percentage, amount of tweets)."
                },
                {
                    "pitch": 5.5,
                    "yaw": -133.0,
                    "type": "info",
                    "text": "Information Panel - Left: Display additional information about the currently selected cluster, for instance the total amount of tweets and time."
                },
                {
                    "pitch": 6.5,
                    "yaw": -50.0,
                    "type": "info",
                    "text": "Information Panel - Right: Display all unique location names that are contained within the currently selected cluster."
                },
                {
                    "pitch": -30.0,
                    "yaw": -90.0,
                    "type": "info",
                    "text": "Selected cluster: The cluster selected by the VR user is outlined with a red wire frame, while all other clusters feature a black wire frame."
                },
                {
                    "pitch": -30.0,
                    "yaw": -49.0,
                    "type": "info",
                    "text": "Cluster: A cluster represents all Twitter traffic within a radius of 60 km as 'stacked cuboids'. Each stacked cuboid consists of four parts: the size of the top three stacks of the cuboid represent, in order, the three most frequent languages detected within the cluster, while the size of the fourth stack represents the sum of all remaining tweets in other languages than the top three."
                },
                {
                    "pitch": -27.0,
                    "yaw": -17.5,
                    "type": "info",
                    "text": "Elevated cluster: Clusters in close proximity will get raised up approximately to the VR user's chest height to facilitate interaction under consideration of human factors and ergonomics. The cluster's shadow is projected to the virtual floor in order to keep a visual indication of its exact location."
                },
                {
                    "pitch": -40.0,
                    "yaw": -35.5,
                    "type": "info",
                    "text": "Nordic countries: The five Nordic countries (Denmark, Finland, Iceland, Norway, Sweden) are displayed, color-coded, on the floor within the virtual three-dimensional (3D) space."
                },
                {
                    "pitch": -15.0,
                    "yaw": 3.0,
                    "type": "info",
                    "text": "Cluster height: Each stacked cuboid's height is scaled according to the total amount of tweets within that cluster (the more tweets, the higher the cuboid, using a logarithmic scale)."
                }
            ];
        }


        // load 360 image
        //

        // determine whether the page is viewn in portrait (typical for mobile) or landscape (typical for desktop) mode
        // and adjust field of view accordingly
        var computedHFOV = (screen.width > screen.height) ? 110 : 40;

        // create 360 viewer and load image
        pannellum.viewer('panorama', {
            "type": "equirectangular",
            "panorama": imageFilePath,
            "autoLoad": true,
            "orientationOnByDefault": true,
            "yaw" : -90,                // default: 0
            "pitch": -10,                 // default: 0
            "hfov": computedHFOV,       // default: 100
            "hotSpots": hotspotData
        });
    </script>
</body>

</html>